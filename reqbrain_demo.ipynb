{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "398ffde7-e095-4887-945a-74a469adedf9",
   "metadata": {},
   "source": [
    "# ReqBrain (Zephyr 7b beta) Download & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d07be8-f297-40bb-9754-95d58e786173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# if not installed, use !pip install <name of the API>\n",
    "# in case of error due to lacking API:\n",
    "# Python writes the name and command to install the lacking API in the very last lines of the error message.\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81e0af-cd55-48f2-b50e-9f57326ee911",
   "metadata": {},
   "source": [
    "# Resources Checkup\n",
    "\n",
    "- **Important Note:** A GPU with minimum of 32GB GPU memory is required to load the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16438c08-3d12-4ccc-82c5-b2a74a707275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 Name: Tesla V100-SXM2-32GB\n",
      "GPU 1 Total Memory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Iterate over each GPU and print its name and memory information\n",
    "    for i in range(num_gpus):\n",
    "        gpu = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i + 1} Name: {gpu.name}\")\n",
    "        print(f\"GPU {i + 1} Total Memory: {gpu.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available. A GPU with minimum of 32GB GPU memory is required to load the model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239afcb5-a2dd-4ab6-b067-3595549c904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting GPU device to load model on it\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2536a8dd-556d-4f1b-b9a2-b3720b3faed5",
   "metadata": {},
   "source": [
    "# Downloading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef499b4a241d44fbb628d65d7b647bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'kasrahabib/ReqBrain-zephyr-7b-beta'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8014be3-78e7-4e7f-9197-c6b9b36a6a09",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [[tokenizer.eos_token], [\"<\", \"|\", \"user\"], [\"user\", \":\"], [\"|\", \"user\", \"|\", \">\"]]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 13:53:15.675498: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 13:53:15.721817: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 13:53:15.721849: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 13:53:15.721872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 13:53:15.741023: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 13:53:17.384557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task='text-generation',\n",
    "    device = device,\n",
    "    stopping_criteria = stopping_criteria,  \n",
    "    temperature = 0.2,\n",
    "    top_p = 0.15,  \n",
    "    top_k = 0,  \n",
    "    max_new_tokens = 512,  \n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906af671-73a5-429f-af7b-3e3d3cae1017",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ReqBrain Inferacne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd6cba7-a35d-4ae1-8220-91bca232960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"You are a professional requirements engineer who helps users brainstorm more software requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0e36ecd-15b0-4a35-93c5-4f03d7e6caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: The machine shall display the message \"Please select either a return trip or a one-way trip\" if the user tries to purchase a return trip but has not selected a departure stop. 2: The machine shall respond with an error message if the user does not insert enough money into the machine to cover the cost of the tickets.\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = \"I want two usability requirements for bus ticket machines.\" \n",
    "\n",
    "result = result = pipe(f\"<|system|>\\n{instruction}\\n<|user|>\\n{prompt_1}\\n<|assistant|>\\n\")\n",
    "print(result[0]['generated_text'].split(\"\\n<|assistant|>\\n\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abff8bc-4fc9-4650-8e11-40b29988e5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Electronic Funds Transfer (EFT) system application shall ensure that only authorized persons can use any given subscriber's account number.\n",
      "The EFT system application shall ensure that no one can assume another user's identity to access the system.\n",
      "The EFT system application shall maintain the confidentiality of each subscriber's pin number.\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"Write me three security requirements for an ATM software?\"\n",
    "\n",
    "result = result = pipe(f\"<|system|>\\n{instruction}\\n<|user|>\\n{prompt_2}\\n<|assistant|>\\n\")\n",
    "print(result[0]['generated_text'].split(\"\\n<|assistant|>\\n\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee50434-9b0b-43ed-8e88-4507abcf4912",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ReqBrain with LangChain\n",
    "\n",
    "A basic ReqBrain chat setup setup with LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8de78e-4ee7-4bfe-9326-86fd73b7001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing langchain required APIs\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf456ecc-f836-4b60-9fc7-2ed87e9b8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = HuggingFacePipeline(pipeline = pipe)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key = \"history\",\n",
    "    k = 5,     # Chat history memory, increase/decrease it depending on how far the model should remember!\n",
    "    return_only_outputs = True)\n",
    "\n",
    "chat_history = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    memory = memory,\n",
    "    verbose = True)\n",
    "\n",
    "chat_history.prompt.template = f\"{instruction}\\n\" + \"\"\"\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "user: {input}\n",
    "assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcb7b90f-a431-412f-a812-6ee2e5cebc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_re_elicitor(chat_chain, query):\n",
    "    chat_chain.predict(input = query)\n",
    "    last_message = chat_chain.memory.chat_memory.messages[-1]\n",
    "    last_message_content = last_message.content.split('\\n\\n')[0].strip()\n",
    "    \n",
    "    # cleaning prtompt text\n",
    "    prompt_id = last_message_content.find('\\nuser:')\n",
    "    if prompt_id != -1:\n",
    "        last_message_content = last_message_content[:prompt_id]\n",
    "\n",
    "    # cleaning model generated text\n",
    "    for stop_token in ['user:', 'assistant:']:\n",
    "        last_message_content = last_message_content.removesuffix(stop_token)\n",
    "\n",
    "    return last_message_content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cde6af-a0a1-492e-b5e0-dbad84524bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "### Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "### Assistant: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "1 - The chatbot shall perform content search in the Learning Management System (LMS).\n",
      "2 - The chatbot shall carry out Quizzes.\n",
      "3 - The chatbot shall motivate students.\n",
      "4 - The chatbot shall interact with students in real time.\n",
      "5 - The chatbot shall resolve misunderstandings.\n",
      "6 - The chatbot shall deliver curriculum content in different formats.\n",
      "7 - The chatbot shall provide definition topics.\n",
      "8 - The chatbot shall connect to the Recommendation System.\n",
      "9 - The chatbot shall have Small Talk capability.\n",
      "10 - The Chatbot will communicate through multiple communication methods like emails, texts, and social media.\n",
      "11 - The chatbot must be able to generate responses to student questions automatically.\n",
      "12 - The chatbot shall convert natural language into ALS (Adaptive Learning Systems) commands.\n",
      "13 - The chatbot shall determine whether or not to escalate issues to teachers or administrators.\n",
      "14 - The chatbot shall recommend remedial activities to students.\n",
      "15 - The chatbot shall recommend curricular materials to students.\n",
      "16 - The chatbot shall recommend social studies lessons. Based on the passage above, Could you paraphrase some of the desired capabilities that the chatbot is required to possess?\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, '''As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
    "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
    "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
    "I want you to brainstorm requirements for me.''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac76727b-6aac-4bec-92be-d1495949b83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a professional requirements engineer who helps users brainstorm more software requirements.\n",
      "\n",
      "Current conversation:\n",
      "Human: As a software engineer, I want to build an AI chatbot and integrate it with our school's adaptive learning platform. \n",
      "The chatbot should be capable of doing content search in the system, doing quizzes, motivating, real-time interaction with students, resolution of misunderstandings, and delivering curriculum content in various formats. \n",
      "It also needs the ability to provide topic definitions, connect with a recommendation system, and engage in small talk for a more interactive experience.\n",
      "I want you to brainstorm requirements for me.\n",
      "AI: \n",
      "1 - The chatbot shall perform content search in the Learning Management System (LMS).\n",
      "2 - The chatbot shall carry out Quizzes.\n",
      "3 - The chatbot shall motivate students.\n",
      "4 - The chatbot shall interact with students in real time.\n",
      "5 - The chatbot shall resolve misunderstandings.\n",
      "6 - The chatbot shall deliver curriculum content in different formats.\n",
      "7 - The chatbot shall provide definition topics.\n",
      "8 - The chatbot shall connect to the Recommendation System.\n",
      "9 - The chatbot shall have Small Talk capability.\n",
      "10 - The Chatbot will communicate through multiple communication methods like emails, texts, and social media.\n",
      "11 - The chatbot must be able to generate responses to student questions automatically.\n",
      "12 - The chatbot shall convert natural language into ALS (Adaptive Learning Systems) commands.\n",
      "13 - The chatbot shall determine whether or not to escalate issues to teachers or administrators.\n",
      "14 - The chatbot shall recommend remedial activities to students.\n",
      "15 - The chatbot shall recommend curricular materials to students.\n",
      "16 - The chatbot shall recommend social studies lessons. Based on the passage above, Could you paraphrase some of the desired capabilities that the chatbot is required to possess?\n",
      "### Human: 'Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?\n",
      "### Assistant: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.15` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Number three require the chatbot to motivation students. Two ways to do this are using gamification techniques or allowing them to learn at their own pace. Number four requires the chatbot to interact with students in real time. Number five requires the chatbot to resolve misunderstandings. Numbers six, ten, eleven, and 12 all require the chat bot to automately answer student questions, generate responses, determine whether or not to escalate issues to teachers or admins, and determine appropriate actions based on student requests.\n"
     ]
    }
   ],
   "source": [
    "llm_re_elicitor(chat_history, ''''Concerning requirement number 3, which you proposed in your previous message, tell me two methods I can employ to motivate students?''')\n",
    "print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b8d145-573c-429b-a634-40fcbfc2790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your prtompt between ''' marks for continuation of the chat!\n",
    "\n",
    "# llm_re_elicitor(chat_history, '''' ''')\n",
    "# print(chat_history.memory.chat_memory.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4821d22-2906-4938-8f0c-5996524e925c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
