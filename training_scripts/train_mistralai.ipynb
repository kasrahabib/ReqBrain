{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EieT2YujHbW"
   },
   "source": [
    "# Setup the Model\n",
    "The following section performs all the setup of the model.\n",
    "This includes\n",
    "\n",
    "- Installing any dependencies\n",
    "- Setting any configuration\n",
    "- Downloading the Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9iaMHIRfNlH"
   },
   "source": [
    "## Install dependencies\n",
    "In order to get started we need to install the appropriate dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLXwJqbjtPho",
    "outputId": "4eda67a0-3fda-4427-e31d-f930817ed831"
   },
   "outputs": [],
   "source": [
    "# # install dependencies\n",
    "\n",
    "# # we use the latest version of transformers, peft, and accelerate\n",
    "# !pip install -q accelerate peft transformers\n",
    "\n",
    "# # install bitsandbytes for quantization\n",
    "# !pip install -q bitsandbytes\n",
    "\n",
    "# # install trl for the SFT library\n",
    "# !pip install -q trl\n",
    "\n",
    "# # we need sentencepiece a slow tokenizer\n",
    "# !pip install sentencepiece\n",
    "\n",
    "# # we need einops, used by falcon-7b, llama-2 etc\n",
    "# # einops (einsteinops) is used to simplify tensorops by making them readable\n",
    "# !pip install -q -U einops\n",
    "\n",
    "# # we need to install datasets for our training dataset\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "loKUs7YRgzKJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"../dataset/\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"ReqBrain-Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-32GB', major=7, minor=0, total_memory=32500MB, multi_processor_count=80)\n",
      "GPU 1: Tesla V100-SXM2-32GB\n",
      "\tCompute Capability: 7.0\n",
      "\tMemory: 31.74 GB\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    \n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    \n",
    "    # List details of each GPU\n",
    "    for i in range(gpu_count):\n",
    "        gpu_device = torch.cuda.get_device_properties(i)\n",
    "        print(gpu_device)\n",
    "        print(f\"GPU {i + 1}: {gpu_device.name}\")\n",
    "        print(f\"\\tCompute Capability: {gpu_device.major}.{gpu_device.minor}\")\n",
    "        print(f\"\\tMemory: {gpu_device.total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2zWMgQ2fUYR"
   },
   "source": [
    "## Download the base model\n",
    "The following will download the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "c3f9aeab4bd0495691e541cf28beb866",
      "fd79da09ec404ecc87778f5ebe785758",
      "42ab92617b04451c80c1ef91f8628b3e",
      "38d0ec959e1a4978b43a46207bc8da72",
      "6c7007f8084d4654acaf826b85212911",
      "77d5963e2f044ea28c4b61c022448025",
      "9db5382331ae490ab9863ed472ef08eb",
      "ab0b8deb74c9416880e31b52a6731492",
      "6092c3a456e04f2b81c968d7fecd8ad2",
      "c07b5defd18f4be09dcdea87179f6da2",
      "a10fe9092daa46549bcde87aab38bd68",
      "95fcee51f14646ffb36703a44cffa9f8",
      "919be25b27924f088083abcd084d4910",
      "351802ad49af4c298ce33bf8437f7582",
      "b38869e14f24454bb9ecba027937898f",
      "fb0ac301ed2344fbb6ff2b0f0a8ef5cc",
      "e08d8e749f3345ed92a5496cf48e5a52",
      "54598edde276432a9844acd01b2afb22",
      "4dccb68292894ee2ba5b0bead19374b0",
      "9023af1898ee4b19a4a9b5f193401b21",
      "b1300f1115f449648931ec3348409621",
      "3c4b4c9d62b6404b9f4d7fa03cbeeab7",
      "e2170cbf8b424528935bf75bc398d838",
      "c8a70bc2a6d74fefaa94deba54f3cc09",
      "cf076c560f8e47808be36614fd3b373c",
      "41235522d87f4225b6eaa45d9b9e860d",
      "a10ee687c9cf44dd87e18020fc980351",
      "e69d616b5ab947459ebf4cfdb74190ce",
      "44f6d781732d44fc807b995f4d11b888",
      "e2b77220d47b4317bcedec55886839dc",
      "4efc5e6beac74528a1b29ed8f93785cd",
      "d57cbae2c0674259aa786ac21096a1bd",
      "9b631532f0dc4724871bcb484ca14bbb",
      "33a0473a63804208b7acb28f7375b147",
      "c2a7270b891643aca4138bc0679a1cbc",
      "71ac79c8518c42d5bf804577f11e666d",
      "bba51cd685954663ba5ec4caf7e7248b",
      "a78b0b90b954453892aa02d0a7e70a27",
      "256d915500b343709848e7f9c4720a74",
      "20c9beee065d4f6ab7ec7b012209ce8e",
      "dcd3fde9271f4085bd2034be7e386eaa",
      "06433a5036ed4504adf21eeb2513bb02",
      "604923fba8054db195005ea1086cad35",
      "b165a8146aa744f1a1edd880c6f5ba75",
      "7df55f81a64e40058e929948ce53cee9",
      "cd5b28e4a7264509a52c6308a7aebdaf",
      "7bf463421eec4db28a665ba87c7a1fb1",
      "dc4d5e4a3fd6411ab9fead329068e35e",
      "61fb3da41861465f86f1a030fdc20261",
      "2838038ce10f4b8990938ce08a8d6da7",
      "e23746983993461783a4684e8f709f23",
      "4d3fbc89c628407dbfa39528f68a5cde",
      "3ae137184b504e638906cd66ae0f5f6d",
      "7777afb84f2b4cd88876cc230f01bb10",
      "885da1552bee4aaab998b2baa1de94ab",
      "ec1c0c106ca44846a768087b4007c325",
      "fa6ec0f057af4a8982a5e0530e5e914f",
      "f646e78d5c2143a9af2b0a10245979a2",
      "c438dc87a7b0469b9c026827d378ef12",
      "b00b55e2b6e044328bf5d7f94ebf3770",
      "0b301591dcf346a48d3e5979a461b7f2",
      "83313b43864f43fe804f0b87e8c0c61a",
      "066f60070643464bac1f85dc435e1eb0",
      "e719688a89a74768baaa66507e848140",
      "16465a60aa414a7bafb553f59962fe20",
      "9d657815b66c463fb57916d2e05e53ac",
      "c45f7ba99d2447dcae887dd7d2a33c3d",
      "85574aad357443739522dd83f550b6d0",
      "d19e7945626e474d87f0e664045938c7",
      "0e555fe559c74500b10611c04bcf6116",
      "a411bf4f59c744d4b0129034e66bbda8",
      "4830037060c84be3883a375863631094",
      "abb4da6734c04bb7bf1388481e6e908c",
      "ca1fbf6e9e564d4db9fc589b35107bd3",
      "e1cba170c41d4c6e809efa5a97311778",
      "10bf40f3e280411580e32dfcb1764102",
      "4014e5d50b3041fa9c3ddbde0d6ea9b5",
      "091981f3de9d4bf58b01c36c2f501efd",
      "995f87b1f2034d51af9ac706e98849d1",
      "44ee3078c6424c449a7b3884336f49d1",
      "3b6d6f1522144ceab246eafaf62c3680",
      "58dcc76be253476c9d9e3369d60d95fe",
      "430ad577a5da4ba798d0e02159b6cfbd",
      "a753363661dd4f6c989674697870e579",
      "c0e0f95d02b146d1abd3df776a082581",
      "c97048ede78c4d32a99aa05bf8b5b575",
      "b1a337eb47964fc68971c3981febb6e2",
      "6e4c68fa9b864cbcbf86d092b81c3efe",
      "f053440370024d859768efb1e3fffcaa",
      "82ccf1a66c5e4d80a2065f25651352fe",
      "6730f2bd40794c339253331a3c64345d",
      "3de37b803ad44ae29c56b3859f7d63db",
      "60a91ba7dd47492e877a9ff367118afa",
      "2dd40933993c4555a6bbcb1d5102b230",
      "c650843e9f874614835ccb49ce1dae3c",
      "7f52f27aca03417eab5ecfeab7e48fa3",
      "d41344e3934b424ea9973c3a9fc29ae6",
      "e0d582302f094d2caa926891b36d5eff",
      "0d020a75ce5a4ca995f8ea23b1b74adf",
      "78579c2cc09e4b9ba37667b0c9621442",
      "d8459f2922604c59a2abf2a471bde414",
      "6510f8ed7fff4de0a704afb13bb7248f",
      "2f88b2f9259f4f319b523020491d477f",
      "27d260126f5b45d386b6edcbeddebd96",
      "265d06f701ac4e39b9b183b8db59afd0",
      "fa63423f18a841bdbd6ae7af44999a0c",
      "0a8bf8d7b7bb44eb8b88299cd276d391",
      "c852ffd0905a46afb6e1cf151f210dec",
      "70dc3e08fd4345d7a30825eca1aac16e",
      "a6ba50c5793d4ab69c84dc751d535d73",
      "fc9ce3642c7a4177ad468b0548a66f50",
      "1015d7ac6b214864b78796716c1174dc",
      "516f8a09b7c342bfbd8fef51e80520bf",
      "6998aef148f8417691c001889f32f4cc",
      "7afd1b13e0374bc88626b61a04a33993",
      "8bf54e0657d84160aec5b13dd1762d57",
      "33bc397005924f9b9021cfd0226e1b56",
      "fc1086903bce4cb9b588c2eb525dc893",
      "70c3b5a87a07408fbb4be8e8f30df3d4",
      "cced1f5d362241ad952ed9e8616d1ad5",
      "7977223d447b499690c18a839bb2dbe1"
     ]
    },
    "id": "nAMzy_0FtaUZ",
    "outputId": "9fed6cfd-9a8d-491c-f786-a7bc01a4e863"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 18:03:51.393280: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-17 18:03:52.253145: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 18:03:52.253186: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 18:03:52.253217: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 18:03:52.318968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-17 18:04:05.019852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54613a099856453e9ce791276beed71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (mistralai)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLlrVRiWhfKR"
   },
   "source": [
    "# Train the Model\n",
    "The following section is about taking your dataset and then finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOMlPjabk0lO"
   },
   "source": [
    "## Load Dataset\n",
    "The following code will load your dataset, ready to be fine tuned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_toUICSmk8Sc"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def get_dataset_by_model_format(dataset, split, ds_format):\n",
    "    return dataset[split].filter(lambda example: example['ds_format'] == ds_format)\n",
    "\n",
    "# Load the dataset\n",
    "instruct_dataset = datasets.load_from_disk(dataset_name)\n",
    "\n",
    "dataset = get_dataset_by_model_format(instruct_dataset, split = 'train', ds_format = 'llama')\n",
    "dataset_test = get_dataset_by_model_format(instruct_dataset, split = 'test', ds_format = 'llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Could you craft a requirement that includes suggestions or allowances? Remember, these are non-mandatory, non-binding provisions, and should utilize the term 'may' in accordance with ISO 29148 guidelines. [/INST] To settle an exposure, the user may select the optional acceptance that should be used as the basis for the liability calculation for the exposure. </s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSDs1baoklS2"
   },
   "source": [
    "## Fine Tune the Model\n",
    "The following section will take your dataset, and fine tune the model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "02d93e10f0124b2b8c39a02f86892b60",
      "767bb7a75b674c0ba9804db38d6f658f",
      "b3534c9622594c1db57ec1dab2f63d07",
      "eb17874ddcdd44e3aee344940b7eb377",
      "ad8fc23a5d494cbc83ee0eb19a3336b4",
      "554874d610d7454dbd964052722b6642",
      "41fc93743dff44e48f1f47a877f4e302",
      "6ed1176d87764abfaa7845459bfe64a3",
      "2895952a510741969db87013345a9391",
      "8c7fc0673d074d559cbd3c364e60fd21",
      "9c9d18319c1045578b46275627bddcf0"
     ]
    },
    "id": "OJXpOgBFuSrc",
    "outputId": "bc4a2248-2f24-4e26-bb36-5b35c0a4c04e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "/home/st/st_us-051520/st_ac137798/.local/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bf5e17eba14464b6f43dc8dffb44ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='990' max='990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [990/990 25:38, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.115200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.881400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.858400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.536200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.398200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.114200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.108400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.106400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.116300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.093100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.083100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.079400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.074600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.074100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.084400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,      # uses the number of epochs earlier\n",
    "    per_device_train_batch_size=2,          # 4 seems reasonable\n",
    "    gradient_accumulation_steps=2,          # 2 is fine, as we're a small batch\n",
    "    optim=\"paged_adamw_32bit\",              # default optimizer\n",
    "    save_steps=0,                           # we're not gonna save\n",
    "    logging_steps=10,                       # same value as used by Meta\n",
    "    learning_rate=2e-4,                     # standard learning rate\n",
    "    weight_decay=0.001,                     # standard weight decay 0.001\n",
    "    fp16=False,                             # set to true for A100\n",
    "    bf16=False,                             # set to true for A100\n",
    "    max_grad_norm=0.3,                      # standard setting\n",
    "    max_steps=-1,                           # needs to be -1, otherwise overrides epochs\n",
    "    warmup_ratio=0.03,                      # standard warmup ratio\n",
    "    group_by_length=True,                   # speeds up the training\n",
    "    lr_scheduler_type=\"cosine\",           # constant seems better than cosine\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,                # use our lora peft config\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,                    # no max sequence length\n",
    "    tokenizer=tokenizer,                    # use the mistralai tokenizer\n",
    "    args=training_arguments,                # use the training arguments\n",
    "    packing=False,                          # don't need packing\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergin LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crj9svNe4hU5"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir results/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkQCviG0Zta-",
    "outputId": "5c5c2054-7ebd-40b0-e714-2029ad904d28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "10dbe607a78b47f18fdaf5452fddd34c",
      "00cd233855ce49369b9c1e5bd4a2b8eb",
      "07ed841b506b4885b8f209a23e39f883",
      "203596710ccc44a39f5fec7a35178810",
      "eea03818a0d24179944d5746d13ee7c8",
      "41ab603e2f9c49e18fe56f13bf372a06",
      "583d53e6a7b8461b8654f94303cb3880",
      "f727cfd03ef7432b928721c628cd24d8",
      "e4624f1a4e254b96ba773c37908a9fe2",
      "9e88c9fd4f1a4f2db06c38a9054582d5",
      "fd9970de85db4322865629000f163ebc"
     ]
    },
    "id": "QQn30cRtAZ-P",
    "outputId": "f69c3d15-35cd-4f3e-8fc0-ddd7ffd8c167"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295ce653b65442cda27bb87505a8153c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git config --global user.email \"kasrahabib@gmail.com\"\n",
    "! git config --global user.name \"Mohammad Kasra Habib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dc5ab67e394ad48d02f08f2f11b75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afd52e74fa64c9f8b84e46b92878c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8c95de29a04c85896e7e732b4e3b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb44230c9aa40a39b9d56d85ea0f7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a344cf57bfc244a99361729e91c02c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00aa1f1a8e1147c187f2b1976cf44075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kasrahabib/Mistral-7B-Instruct-v0.2-ReqBrain/commit/4dd9963faa287318443e87832afeec6971fc458e', commit_message='Upload tokenizer', commit_description='', oid='4dd9963faa287318443e87832afeec6971fc458e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "H9iaMHIRfNlH",
    "e2zWMgQ2fUYR",
    "E5pqSD7xiwlK",
    "OOMlPjabk0lO"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
