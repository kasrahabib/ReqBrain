{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49be96bd-de36-48cd-8ecd-4f6048a8b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b304279-e44b-41e4-9e9b-15aafc1572d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0d9c7b8a2a4bbf8505a9412a5266e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'kasrahabib/ReqBrain-Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcdb1fb-447a-4033-81e1-2ff0bde8487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "___double_back_slash = tokenizer.convert_ids_to_tokens(tokenizer(\"\\\\\\\\\")[\"input_ids\"])[1:]\n",
    "___double_forward_slash = tokenizer.convert_ids_to_tokens(tokenizer(\"//\")[\"input_ids\"])[1:]\n",
    "___single_forward_slash = tokenizer.convert_ids_to_tokens(tokenizer(\"/\")[\"input_ids\"])[1:]\n",
    "___single_back_slash = tokenizer.convert_ids_to_tokens(tokenizer(\"\\\\\")[\"input_ids\"])[1:]\n",
    "stop_token_ids = [\n",
    "    tokenizer.convert_tokens_to_ids(x) for x in [[\"INST\"], [tokenizer.eos_token], [\"end\", \"of\", \"list\"], [\"end\", \"_\", \"of\", \"_\"], [\"/\", \"End\"], ___double_back_slash, ___double_forward_slash, ___single_forward_slash, ___single_back_slash]\n",
    "]\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106fa892-abd9-49fa-9dd3-4a27457a9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c50a943-e82d-441b-a3ff-5aaa443b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 17:17:32.683030: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-19 17:17:32.718873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-19 17:17:32.718907: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-19 17:17:32.718930: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-19 17:17:32.726855: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 17:17:37.347259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    return_full_text = True, # Set it to True when combining with LangChain\n",
    "    task='text-generation',\n",
    "    device=device,\n",
    "    stopping_criteria=stopping_criteria,  \n",
    "    temperature = 0.1,\n",
    "    top_p=0.15,  \n",
    "    top_k=0,  \n",
    "    max_new_tokens = 512,  \n",
    "    repetition_penalty = 1.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04d321e-d847-4c13-b30c-99fa1b85f58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['REQID_ex', 'completion', 'query', 'class', 'task', 'text', 'label', 'falcone_7b_base_preds', 'falcon_7b_instruct_preds', 'llama2_7b_chat_hf_preds'],\n",
       "    num_rows: 34\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "evaluation_set = datasets.load_from_disk(\"./models_prediction_dataset\")\n",
    "evaluation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e633736a-530e-4713-9beb-77f93c93de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|▎         | 1/34 [00:03<01:49,  3.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  6%|▌         | 2/34 [00:08<02:13,  4.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|▉         | 3/34 [00:12<02:12,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█▏        | 4/34 [00:17<02:14,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|█▍        | 5/34 [00:38<05:10, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|█▊        | 6/34 [01:00<06:40, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 21%|██        | 7/34 [01:08<05:33, 12.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██▎       | 8/34 [01:11<04:03,  9.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|██▋       | 9/34 [01:32<05:27, 13.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 29%|██▉       | 10/34 [01:35<03:56,  9.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|███▏      | 11/34 [01:36<02:43,  7.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███▌      | 12/34 [01:57<04:11, 11.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|███▊      | 13/34 [02:04<03:32, 10.14s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|████      | 14/34 [02:09<02:46,  8.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 44%|████▍     | 15/34 [02:13<02:16,  7.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████▋     | 16/34 [02:14<01:35,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████     | 17/34 [02:26<02:04,  7.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|█████▎    | 18/34 [02:47<03:04, 11.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 56%|█████▌    | 19/34 [03:03<03:12, 12.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████▉    | 20/34 [03:09<02:29, 10.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████▏   | 21/34 [03:10<01:42,  7.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|██████▍   | 22/34 [03:14<01:18,  6.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|██████▊   | 23/34 [03:16<00:57,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 71%|███████   | 24/34 [03:23<00:59,  5.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████▎  | 25/34 [03:26<00:44,  4.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|███████▋  | 26/34 [03:35<00:49,  6.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 79%|███████▉  | 27/34 [03:38<00:37,  5.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|████████▏ | 28/34 [03:45<00:34,  5.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████▌ | 29/34 [03:53<00:31,  6.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|████████▊ | 30/34 [03:56<00:21,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|█████████ | 31/34 [03:57<00:12,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 94%|█████████▍| 32/34 [04:18<00:18,  9.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████▋| 33/34 [04:22<00:07,  7.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 34/34 [04:43<00:00,  8.35s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "completion = []\n",
    "\n",
    "for query in tqdm(evaluation_set['query']):\n",
    "    result = pipe(f\"<s>[INST] {query} [/INST]\")\n",
    "    result = result[0]['generated_text'].split('[/INST]')[-1].strip(\"\\\\\\\\\")\n",
    "    result = result.strip(\"//\")\n",
    "    result = result.strip(\"/\")\n",
    "    result = result.strip(\"\\\\\")\n",
    "    result = result.strip(\"/End\")\n",
    "    result = result.strip(\" \")\n",
    "    completion.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5a61a77-5e8b-486b-b484-f9eb6e34d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_set = evaluation_set.add_column(\"mistral_ai_instruct_7b_chat_hf_preds\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0bf7ba-4515-46f6-ba3a-f1bce1f494d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2693b7c2734d378df325127ba950b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_set.save_to_disk(\"./models_prediction_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
