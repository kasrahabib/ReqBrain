{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de20d6fa-b40c-4f2d-999f-a99404e841e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01c504-1973-4d82-be87-cf47f78dc2bc",
   "metadata": {},
   "source": [
    "# Loading the Instruct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2109b1-99d4-4d7e-9c46-9a242a99bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['REQID_ex', 'completion', 'query', 'class', 'task', 'text', 'label', 'mistral_ai_instruct_7b_chat_hf_preds', 'falcon_7b_base_preds', 'falcon_7b_instruct_preds', 'llama2_7b_chat_hf_preds', 'zephyr_7b_beta_preds', 'openai_compe_gpt4o_24_11_20', 'chatgpt4o_frugal_score', 'chatgpt4o_bert_score', 'zephyr_frugal_score', 'zephyr_bert_score', 'mistralai_frugal_score', 'mistralai_bert_score', 'falcon_base_frugal_score', 'falcon_base_bert_score', 'falcon_frugal_score', 'falcon_bert_score', 'llama_frugal_score', 'llama_bert_score'],\n",
       "    num_rows: 34\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_set = datasets.load_from_disk('./evaluation_set_for_nlp_metrics/models_prediction_dataset')\n",
    "evaluation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad43501e-51bd-475a-8533-d0c917e37eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# splitting the human written requirements\n",
    "\n",
    "references = evaluation_set['completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39293bd9-1a97-4c0d-ac5f-18f7d3b81db0",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8755cc-683f-4595-897d-31ab7174e6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(references, predictions):\n",
    "    # Setup BERT Score\n",
    "    bertscore = evaluate.load('bertscore')\n",
    "    bertscore_results = bertscore.compute(predictions = predictions, references = references, model_type = \"xlm-mlm-en-2048\", lang = 'en')\n",
    "    \n",
    "    # Setup FRUGAL Score\n",
    "    frugalscore = evaluate.load(\"frugalscore\", \"moussaKam/frugalscore_medium_roberta_bert-score\")\n",
    "    frugalscore_results = frugalscore.compute(predictions=predictions, references=references, batch_size = 2, max_length = 512, device = \"cpu\")\n",
    "    \n",
    "    return {'bert_score': bertscore_results, 'frugal_score': frugalscore_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0516fa23-2bff-4c08-977b-05dda428284a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_model_evaluation_results(results):\n",
    "    # Display BERT Score\n",
    "    print('\\033[1m BERT Score: \\033[0m')\n",
    "    for metric in list(results['bert_score'].keys())[:-1]:\n",
    "        pairwise_metric = results['bert_score'][metric]\n",
    "        averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "        print(f'   {metric}:   ', averaged_metric)\n",
    "    \n",
    "    # Display FRUGAL\n",
    "    print('\\033[1m FRUGAL Score: \\033[0m')\n",
    "    pairwise_frugal_score = results['frugal_score']['scores']\n",
    "    averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "    print('   Score:   ', averaged_frugal_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860d064-e6ab-40a2-9698-a90afd83d104",
   "metadata": {},
   "source": [
    "# Evaluating ChatGPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fb18cf-e011-4af5-9e35-dd041bebbdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammadkasrahabib/miniforge3/envs/metal-engine/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/Users/mohammadkasrahabib/miniforge3/envs/metal-engine/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5dad24aca5428bad25e6dde12cd933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammadkasrahabib/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--frugalscore/3e67da829730648355c5bb58bc8b93a6c3a9be29cf03f72f71a8fc1bdf7614a6/frugalscore.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(self.model, training_args, tokenizer=self.tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_gpt_4o_results = evaluate_model(references, evaluation_set['openai_compe_gpt4o_24_11_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6b9a8f-b4fe-4d99-897d-f7aa8cf03122",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BERT Score: \u001b[0m\n",
      "   precision:    0.8187058673185461\n",
      "   recall:    0.883557461640414\n",
      "   f1:    0.8497847266056958\n",
      "\u001b[1m FRUGAL Score: \u001b[0m\n",
      "   Score:    0.865664145525764\n"
     ]
    }
   ],
   "source": [
    "display_model_evaluation_results(chat_gpt_4o_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af444a8-d0b9-4f57-bd6b-90b4fc24e03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing model results in a new column for paper visualization\n",
    "\n",
    "# evaluation_set = evaluation_set.add_column('chatgpt4o_frugal_score', chat_gpt_4o_results['frugal_score']['scores'])\n",
    "# evaluation_set = evaluation_set.add_column('chatgpt4o_bert_score', chat_gpt_4o_results['bert_score']['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec273d9-6f85-42b2-b871-d52525b70d87",
   "metadata": {},
   "source": [
    "# Evaluating ReqBrain-zephyr-7b-beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91642738-b8b7-47ee-a1ed-3c7730a289d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/mohammadkasrahabib/miniforge3/envs/metal-engine/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1c5885385a412799c73a576a192949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammadkasrahabib/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--frugalscore/3e67da829730648355c5bb58bc8b93a6c3a9be29cf03f72f71a8fc1bdf7614a6/frugalscore.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(self.model, training_args, tokenizer=self.tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "zephyr_7b_beta_results = evaluate_model(references, evaluation_set['zephyr_7b_beta_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d1c5ec2-71eb-4828-8925-c3d9885f3b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BERT Score: \u001b[0m\n",
      "   precision:    0.8904935454621035\n",
      "   recall:    0.8960548902259153\n",
      "   f1:    0.8930980773533091\n",
      "\u001b[1m FRUGAL Score: \u001b[0m\n",
      "   Score:    0.9120348762063419\n"
     ]
    }
   ],
   "source": [
    "display_model_evaluation_results(zephyr_7b_beta_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f9b4f9-9921-466b-8195-5b7da0c439db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing model results in a new column for paper visualization\n",
    "\n",
    "# evaluation_set = evaluation_set.add_column('zephyr_frugal_score', zephyr_7b_beta_results['frugal_score']['scores'])\n",
    "# evaluation_set = evaluation_set.add_column('zephyr_bert_score', zephyr_7b_beta_results['bert_score']['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6eeed5-5555-416f-b4fe-683158af6ad4",
   "metadata": {},
   "source": [
    "# Evaluating ReqBrain-Mistral-7B-Instruct-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f39213f-e644-4593-b33f-9822e7270354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85bfc9b855234d5fa337079c0eb01ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mistralai_7b_instruct_results = evaluate_model(references, evaluation_set['mistral_ai_instruct_7b_chat_hf_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c94bd65b-427f-4c06-9e61-33bc60f04005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BERT Score: \u001b[0m\n",
      "   precision:    0.8448562376639422\n",
      "   recall:    0.8912202593158273\n",
      "   f1:    0.8671604289728052\n",
      "\u001b[1m FRUGAL Score: \u001b[0m\n",
      "   Score:    0.888142417458927\n"
     ]
    }
   ],
   "source": [
    "display_model_evaluation_results(mistralai_7b_instruct_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "008a81c4-985f-4160-9570-ab4c617bda88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing model results in a new column for paper visualization\n",
    "\n",
    "# evaluation_set = evaluation_set.add_column('mistralai_frugal_score', mistralai_7b_instruct_results['frugal_score']['scores'])\n",
    "# evaluation_set = evaluation_set.add_column('mistralai_bert_score', mistralai_7b_instruct_results['bert_score']['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ce1e5-dced-4005-bb9c-845499cca589",
   "metadata": {},
   "source": [
    "# Evaluating ReqBrain-falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18ade02-d081-48ae-a928-7637ab4ad8df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c71e5225df748a99ed285cdae2d76e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "falcon_7b_results = evaluate_model(references, evaluation_set['falcon_7b_base_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae462d84-dac2-4ea4-907a-088875d56ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BERT Score: \u001b[0m\n",
      "   precision:    0.8033859992728514\n",
      "   recall:    0.8234570236767039\n",
      "   f1:    0.8587363923297209\n",
      "\u001b[1m FRUGAL Score: \u001b[0m\n",
      "   Score:    0.8855502184699563\n"
     ]
    }
   ],
   "source": [
    "display_model_evaluation_results(falcon_7b_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c08ecc3e-9fed-4386-84f7-ddece6a32f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing model results in a new column for paper visualization\n",
    "\n",
    "# evaluation_set = evaluation_set.add_column('falcon_base_frugal_score', falcon_7b_results['frugal_score']['scores'])\n",
    "# evaluation_set = evaluation_set.add_column('falcon_base_bert_score', falcon_7b_results['bert_score']['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7c0b6-cd64-4ddc-88f1-534c7c9bb035",
   "metadata": {},
   "source": [
    "# Evaluating ReqBrain-falcon-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d95274be-6df8-40bb-8615-cecfb05d8290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3888e085b7fc4b7db9ad4d04a5a27040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "falcon_7b_instruct_results = evaluate_model(references, evaluation_set['falcon_7b_instruct_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f2634d4-65b4-4728-be36-ec09bf6a73b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BERT Score: \u001b[0m\n",
      "   precision:    0.8550121696556315\n",
      "   recall:    0.8839603189159843\n",
      "   f1:    0.8689493677195381\n",
      "\u001b[1m FRUGAL Score: \u001b[0m\n",
      "   Score:    0.8859289393705481\n"
     ]
    }
   ],
   "source": [
    "display_model_evaluation_results(falcon_7b_instruct_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66b6087d-d9f6-48ab-ad83-9eb3d496894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing model results in a new column for paper visualization\n",
    "\n",
    "# evaluation_set = evaluation_set.add_column('falcon_frugal_score', falcon_7b_instruct_results['frugal_score']['scores'])\n",
    "# evaluation_set = evaluation_set.add_column('falcon_bert_score', falcon_7b_instruct_results['bert_score']['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22056cf-7876-48f0-b9e8-2ae650dc0534",
   "metadata": {},
   "source": [
    "# Evaluating ReqBrain-Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34fbe6ab-8314-4fba-877d-8d82b3fa31de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ba9a1d09894504b3374460d9f3c1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_2_7b_chat_hf_results = evaluate_model(references, evaluation_set['llama2_7b_chat_hf_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d811be67-2c3e-4277-a506-8b20b1d60c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BERT Score: \u001b[0m\n",
      "   precision:    0.8162222627331229\n",
      "   recall:    0.8587226306690889\n",
      "   f1:    0.8597163996275734\n",
      "\u001b[1m FRUGAL Score: \u001b[0m\n",
      "   Score:    0.8812636768116671\n"
     ]
    }
   ],
   "source": [
    "display_model_evaluation_results(llama_2_7b_chat_hf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeeae21e-4c63-4a93-91e7-fb82fc305d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing model results in a new column for paper visualization\n",
    "\n",
    "# evaluation_set = evaluation_set.add_column('llama_frugal_score', llama_2_7b_chat_hf_results['frugal_score']['scores'])\n",
    "# evaluation_set = evaluation_set.add_column('llama_bert_score', llama_2_7b_chat_hf_results['bert_score']['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78aa7705-fbb1-4aa1-8bb7-ccecc8a9b97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # saving dataset with special columns for spider chart\n",
    "\n",
    "# evaluation_set.save_to_disk('./evaluation_set_for_nlp_metrics/models_prediction_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal-engine",
   "language": "python",
   "name": "metal-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
