{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f295e5-f16e-4343-a733-ef61a294446d",
   "metadata": {},
   "source": [
    "<a href=\"\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927387e-4ceb-494c-88be-28c1a875be92",
   "metadata": {},
   "source": [
    "# Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de20d6fa-b40c-4f2d-999f-a99404e841e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01c504-1973-4d82-be87-cf47f78dc2bc",
   "metadata": {},
   "source": [
    "# Loading the Instruct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b2109b1-99d4-4d7e-9c46-9a242a99bb08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['REQID_ex', 'completion', 'query', 'class', 'task', 'text', 'label', 'mistral_ai_instruct_7b_chat_hf_preds', 'falcon_7b_base_preds', 'falcon_7b_instruct_preds', 'llama2_7b_chat_hf_preds', 'zephyr_7b_beta_preds', 'open_ai_gpt4', 'openai_compe_gpt4o_24_11_20'],\n",
       "    num_rows: 34\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_from_disk('./predicting_with_models/models_prediction_dataset')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad43501e-51bd-475a-8533-d0c917e37eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# splitting the human written requirements\n",
    "\n",
    "references = dataset['completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39293bd9-1a97-4c0d-ac5f-18f7d3b81db0",
   "metadata": {},
   "source": [
    "# Putting all Metrics Togather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f8755cc-683f-4595-897d-31ab7174e6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(references, predictions):\n",
    "    bleu = evaluate.load('sacrebleu')\n",
    "    rouge = evaluate.load('rouge')\n",
    "    ter = evaluate.load(\"ter\")\n",
    "    bertscore = evaluate.load('bertscore')\n",
    "    frugalscore = evaluate.load(\"frugalscore\", \"moussaKam/frugalscore_medium_roberta_bert-score\")\n",
    "\n",
    "    bleu_results = bleu.compute(predictions = predictions, references = references)\n",
    "    rouge_results = rouge.compute(predictions = predictions, references = references)\n",
    "    ter_results = ter.compute(predictions = predictions, references = references) # case_sensitive = False, ignore_punct = True\n",
    "    bertscore_results = bertscore.compute(predictions = predictions, references = references, model_type = \"xlm-mlm-en-2048\", lang = 'en')\n",
    "    frugalscore_results = frugalscore.compute(predictions=predictions, references=references, batch_size = 2, max_length = 512, device = \"cpu\")\n",
    "    return {'bleu': bleu_results, 'rouge': rouge_results, 'ter': ter_results, 'bert_score': bertscore_results, 'frugal_score': frugalscore_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae50112-2c1a-422c-ae4f-f3a66871f822",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating Single Requirements Zephyr Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89b23203-b610-4252-b016-245c6f273c61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me an availability requirement that outlines yearly 24/7 uptime for the website for the project.\n",
      "******************************\n",
      "Original:  The website shall be available for use 24 hours per day 365 days per year.\n",
      "Generated:  The website shall be available 24 hours a day, 365 days per year except for scheduled maintenance days.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbf2112dc79448ea68e4aeb2697ca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 34.6697783111003\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.7878787878787877\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.5806451612903226\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.7272727272727272\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.7272727272727272\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 60.0\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 9\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 15.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.9161564707756042\n",
      "\u001b[1m recall:\u001b[0m \t 0.9383234977722168\n",
      "\u001b[1m f1:\u001b[0m \t 0.9271074533462524\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.9607145190238953\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print(dataset['query'][10])\n",
    "print(\"*\" * 30) \n",
    "print(\"Original: \", dataset['completion'][10])\n",
    "print(\"Generated: \", dataset['zephyr_7b_beta_preds'][10])\n",
    "\n",
    "results = evaluate_model([dataset['completion'][10]], [dataset['zephyr_7b_beta_preds'][10]])\n",
    "\n",
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646fcd5-c52a-40d7-b2e2-161482e5fa1c",
   "metadata": {},
   "source": [
    "# Evaluating Trained Models using Five NLP Human Correlation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0870629-3f5b-444c-b018-c1b867cd08d7",
   "metadata": {},
   "source": [
    "## Evaluating GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25bba0ed-f8a7-43b6-b9a2-d214c9066cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7173a2aba5d84ceaa043c31b7d42cfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['open_ai_gpt4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52484bcd-e4f4-49b5-b02a-783ebf8fe18c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "gpt4_frugal_score = results['frugal_score']['scores']\n",
    "gpt4_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset.add_column('gpt_4_frugal_score', gpt4_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('gpt4_bert_score', gpt4_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "344e94f2-dd56-4ca2-ba72-c7a24fe9dcba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 3.217775308187738\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.21219613842714666\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.09392426726748879\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.16818299252459562\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.17634244553547063\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 537.129300118624\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 4528\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8391319986651925\n",
      "\u001b[1m recall:\u001b[0m \t 0.8900714709478266\n",
      "\u001b[1m f1:\u001b[0m \t 0.8636762079070596\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8817797829123104\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860d064-e6ab-40a2-9698-a90afd83d104",
   "metadata": {},
   "source": [
    "## Evaluating GPT4o-2024-11-20 (latest as of 21-12.2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64fb18cf-e011-4af5-9e35-dd041bebbdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa19efe13054765b91b72e3fe00e2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['openai_compe_gpt4o_24_11_20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9af444a8-d0b9-4f57-bd6b-90b4fc24e03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "gpt4_frugal_score = results['frugal_score']['scores']\n",
    "gpt4_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('openai_compe_gpt4o_24_11_20_frugal_score', gpt4_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('openai_compe_gpt4o_24_11_20_bert_score', gpt4_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea0b341d-8196-4699-81b8-4bb5ac5a9e13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 2.0219287149549072\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.1583864937577551\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.06966690199808939\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.1266496459627222\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.1347586325676159\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 762.0403321470937\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 6424\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8187058673185461\n",
      "\u001b[1m recall:\u001b[0m \t 0.883557461640414\n",
      "\u001b[1m f1:\u001b[0m \t 0.8497847266056958\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.865664145525764\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec273d9-6f85-42b2-b871-d52525b70d87",
   "metadata": {},
   "source": [
    "## Evaluating Zephyr 7b beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91642738-b8b7-47ee-a1ed-3c7730a289d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5aef731fd242a2a168eb8de6c73c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['zephyr_7b_beta_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04f9b4f9-9921-466b-8195-5b7da0c439db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "zephyr_frugal_score = results['frugal_score']['scores']\n",
    "zephyr_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('zephyr_frugal_score', zephyr_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('zephyr_bert_score', zephyr_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d1c5ec2-71eb-4828-8925-c3d9885f3b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 12.264924145523484\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.42851236439172186\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.20262459923753579\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.3674005948889094\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.3667809761480655\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 108.89679715302492\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 918\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8904935454621035\n",
      "\u001b[1m recall:\u001b[0m \t 0.8960548902259153\n",
      "\u001b[1m f1:\u001b[0m \t 0.8930980773533091\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.9120348762063419\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f39213f-e644-4593-b33f-9822e7270354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e72d92dea41048e039b50f5414192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['mistral_ai_instruct_7b_chat_hf_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "008a81c4-985f-4160-9570-ab4c617bda88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "mistralai_frugal_score = results['frugal_score']['scores']\n",
    "mistralai_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('mistralai_frugal_score', mistralai_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('mistralai_bert_score', mistralai_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c94bd65b-427f-4c06-9e61-33bc60f04005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 3.1655487335449854\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.24405415489707774\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.1074544050386387\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.19902962294033172\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.21559164497381814\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 487.4258600237248\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 4109\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8448562376639422\n",
      "\u001b[1m recall:\u001b[0m \t 0.8912202593158273\n",
      "\u001b[1m f1:\u001b[0m \t 0.8671604289728052\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.888142417458927\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ce1e5-dced-4005-bb9c-845499cca589",
   "metadata": {},
   "source": [
    "## Evaluating Falcon Base 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e18ade02-d081-48ae-a928-7637ab4ad8df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a737b88a4dde4d5ab92d106b1678d1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['falcon_7b_base_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c08ecc3e-9fed-4386-84f7-ddece6a32f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "falcon_base_frugal_score = results['frugal_score']['scores']\n",
    "falcon_base_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_base_frugal_score', falcon_base_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_base_bert_score', falcon_base_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae462d84-dac2-4ea4-907a-088875d56ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 2.263719074338972\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.2387465003662007\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.07281206093697933\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.1989111532743889\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.20628888113524013\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 290.98457888493476\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 2453\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8033859992728514\n",
      "\u001b[1m recall:\u001b[0m \t 0.8234570236767039\n",
      "\u001b[1m f1:\u001b[0m \t 0.8587363923297209\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8855502184699563\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7c0b6-cd64-4ddc-88f1-534c7c9bb035",
   "metadata": {},
   "source": [
    "## Evaluating Falcon Instruct 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d95274be-6df8-40bb-8615-cecfb05d8290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c073ac70d0fb4dc1aacb0429e876d919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['falcon_7b_instruct_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66b6087d-d9f6-48ab-ad83-9eb3d496894a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "falcon_frugal_score = results['frugal_score']['scores']\n",
    "falcon_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_frugal_score', falcon_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('falcon_bert_score', falcon_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f2634d4-65b4-4728-be36-ec09bf6a73b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 3.0469204599271507\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.2837586315727416\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.1123260649543652\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.2351724241542409\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.23719027391774433\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 303.7959667852906\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 2561\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8550121696556315\n",
      "\u001b[1m recall:\u001b[0m \t 0.8839603189159843\n",
      "\u001b[1m f1:\u001b[0m \t 0.8689493677195381\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8859289393705481\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22056cf-7876-48f0-b9e8-2ae650dc0534",
   "metadata": {},
   "source": [
    "## Evaluating Llama2 7b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34fbe6ab-8314-4fba-877d-8d82b3fa31de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc96179d6884649b6ce5e84067905f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = evaluate_model(references, dataset['llama2_7b_chat_hf_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aeeae21e-4c63-4a93-91e7-fb82fc305d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to generate a two formated columns for BERT and FRUGAL to be used for SPIDER chart on paper\n",
    "\n",
    "llama_frugal_score = results['frugal_score']['scores']\n",
    "llama_bert_score = results['bert_score']['f1']\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('llama_frugal_score', llama_frugal_score)\n",
    "dataset_for_spider_chart = dataset_for_spider_chart.add_column('llama_bert_score', llama_bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d811be67-2c3e-4277-a506-8b20b1d60c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m BLEU Score:\u001b[0m \t 2.348697647210191\n",
      "......................................................................................................................................................\n",
      "\u001b[1m rouge1:\u001b[0m \t 0.2388015061693396\n",
      "\u001b[1m rouge2:\u001b[0m \t 0.0940685502709552\n",
      "\u001b[1m rougeL:\u001b[0m \t 0.18948837289602968\n",
      "\u001b[1m rougeLsum:\u001b[0m \t 0.1985124547383369\n",
      "......................................................................................................................................................\n",
      "\u001b[1m TER Score:\u001b[0m \t 466.54804270462637\n",
      "\u001b[1m TER #Edits:\u001b[0m \t 3933\n",
      "\u001b[1m TER Ref. Length:\u001b[0m \t 843.0\n",
      "......................................................................................................................................................\n",
      "\u001b[1m precision:\u001b[0m \t 0.8162222627331229\n",
      "\u001b[1m recall:\u001b[0m \t 0.8587226306690889\n",
      "\u001b[1m f1:\u001b[0m \t 0.8597163996275734\n",
      "......................................................................................................................................................\n",
      "\u001b[1m FRUGAL Score:\u001b[0m \t 0.8812636768116671\n",
      "......................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m BLEU Score:\\033[0m \\t', results['bleu']['score'])\n",
    "print('.' * 150)\n",
    "\n",
    "for score in list(results['rouge'].keys()):\n",
    "    print(f'\\033[1m {score}:\\033[0m \\t', results['rouge'][score])\n",
    "print('.' * 150)\n",
    "\n",
    "print('\\033[1m TER Score:\\033[0m \\t', results['ter']['score'])\n",
    "print('\\033[1m TER #Edits:\\033[0m \\t', results['ter']['num_edits'])\n",
    "print('\\033[1m TER Ref. Length:\\033[0m \\t', results['ter']['ref_length'])\n",
    "print('.' * 150)\n",
    "\n",
    "for metric in list(results['bert_score'].keys())[:-1]:\n",
    "    pairwise_metric = results['bert_score'][metric]\n",
    "    averaged_metric = np.sum(pairwise_metric)/len(pairwise_metric)\n",
    "    print(f'\\033[1m {metric}:\\033[0m \\t', averaged_metric)\n",
    "print('.' * 150)\n",
    "\n",
    "pairwise_frugal_score = results['frugal_score']['scores']\n",
    "averaged_frugal_score = np.sum(results['frugal_score']['scores'])/len(results['frugal_score']['scores'])\n",
    "print('\\033[1m FRUGAL Score:\\033[0m \\t', averaged_frugal_score)\n",
    "print('.' * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78aa7705-fbb1-4aa1-8bb7-ccecc8a9b97b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c58fc2186e4fd8b9684c9c37ce990f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/34 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving dataset with special columns for spider chart\n",
    "dataset_for_spider_chart.save_to_disk('./dataset_for_spider_chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d2cc9-e869-49f6-8bb3-657706705cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d9de8-7755-4aa2-a468-b543fa31bc74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal-engine",
   "language": "python",
   "name": "metal-engine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
